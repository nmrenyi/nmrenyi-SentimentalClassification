{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 18330,
     "status": "ok",
     "timestamp": 1590419364573,
     "user": {
      "displayName": "任一",
      "photoUrl": "",
      "userId": "04098430598358376154"
     },
     "user_tz": -480
    },
    "id": "F-XqBMN29MCK",
    "outputId": "9ab079bc-7da2-4949-9788-a0af3a4b04f4"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from itertools import chain\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.utils import shuffle\n",
    "from time import time\n",
    "torch.manual_seed(991006)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rgcuHBWN9MCO"
   },
   "outputs": [],
   "source": [
    "train_df = shuffle(pd.read_csv('./data/data_train.csv'))\n",
    "test_df = pd.read_csv('./data/data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jEAhRoh-9MCQ"
   },
   "outputs": [],
   "source": [
    "train_corpus = [x.split() for x in train_df['Text'].to_list()]\n",
    "train_labels = train_df['MaxLabel'].to_list()\n",
    "test_corpus =  [x.split() for x in test_df['Text'].to_list()]\n",
    "test_labels = test_df['MaxLabel'].to_list()\n",
    "test_labels_distribution = test_df.iloc[:, 3:11].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21277,
     "status": "ok",
     "timestamp": 1590419367543,
     "user": {
      "displayName": "任一",
      "photoUrl": "",
      "userId": "04098430598358376154"
     },
     "user_tz": -480
    },
    "id": "ubRVOZBc9MCV",
    "outputId": "98df950d-36f4-4dbb-b88a-590dd15c19ae"
   },
   "outputs": [],
   "source": [
    "train_corpus_max_length = np.max(([len(x) for x in train_corpus]))\n",
    "# padding all sentences to the same length(i.e. the max sentence length in the corpus)\n",
    "for s in train_corpus:\n",
    "    sentence_length = len(s)\n",
    "    for i in range(train_corpus_max_length - sentence_length):\n",
    "        s.append('')\n",
    "train_sentence_num = len(train_corpus)\n",
    "train_sentence_length = len(train_corpus[0])\n",
    "print('train sentence count:', train_sentence_num)\n",
    "print('train sentence length:', train_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KqsjwChP9MCb"
   },
   "outputs": [],
   "source": [
    "# index the words\n",
    "word_to_ix = np.load('./data/word_to_ix.npy', allow_pickle=True).item() # index the words\n",
    "vocab_size = len(set(chain.from_iterable(train_corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14574,
     "status": "ok",
     "timestamp": 1590419457320,
     "user": {
      "displayName": "任一",
      "photoUrl": "",
      "userId": "04098430598358376154"
     },
     "user_tz": -480
    },
    "id": "Mky5fO8t9MCi",
    "outputId": "fbbd0e06-cf65-45be-a9ab-03929fad0764"
   },
   "outputs": [],
   "source": [
    "train_index = torch.zeros([0, train_sentence_length], dtype=torch.long)\n",
    "for s in train_corpus:\n",
    "    ids = torch.tensor([word_to_ix[w] for w in s], dtype=torch.long)\n",
    "    ids = ids.unsqueeze(dim = 0)\n",
    "    train_index = torch.cat((train_index, ids), dim = 0)\n",
    "train_index = train_index.long()[:, 0:1000]\n",
    "train_index.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14562,
     "status": "ok",
     "timestamp": 1590419457321,
     "user": {
      "displayName": "任一",
      "photoUrl": "",
      "userId": "04098430598358376154"
     },
     "user_tz": -480
    },
    "id": "w-EjLtCQ9MCl",
    "outputId": "a43a6aeb-1477-4fad-9a55-8f0254facf36"
   },
   "outputs": [],
   "source": [
    "test_corpus_max_length = np.max([len(x) for x in test_corpus])\n",
    "# padding all sentences to the same length(i.e. the max sentence length in the corpus)\n",
    "for s in test_corpus:\n",
    "    sentence_length = len(s)\n",
    "    for i in range(test_corpus_max_length - sentence_length):\n",
    "        s.append('')\n",
    "test_sentence_num = len(test_corpus)\n",
    "test_sentence_length = len(test_corpus[0])\n",
    "print('test sentence count:', test_sentence_num)\n",
    "print('test sentence length:', test_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30010,
     "status": "ok",
     "timestamp": 1590419472779,
     "user": {
      "displayName": "任一",
      "photoUrl": "",
      "userId": "04098430598358376154"
     },
     "user_tz": -480
    },
    "id": "Zke6Uzkr9MCo",
    "outputId": "218b6f4c-5877-4ef2-9f13-5704bc30a2fd",
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "test_index = torch.zeros([0, test_sentence_length], dtype=torch.long)\n",
    "for s in test_corpus:\n",
    "    ids_list = list()\n",
    "    not_found_num = 0\n",
    "    for w in s:\n",
    "        if w in word_to_ix:\n",
    "            ids_list.append(word_to_ix[w])\n",
    "        else:\n",
    "            not_found_num += 1\n",
    "            # ids_list.append(word_to_ix[''])\n",
    "    for i in range(not_found_num):\n",
    "      ids_list.append(word_to_ix[''])\n",
    "    ids = torch.tensor(ids_list, dtype=torch.long)\n",
    "    ids = ids.unsqueeze(dim = 0)\n",
    "    test_index = torch.cat((test_index, ids), dim = 0)\n",
    "test_index = test_index.long()[:, 0:1000]\n",
    "test_index.size()\n",
    "print('not found num:', not_found_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tpjyJIW69MCq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeNCJtr89MCs"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "train_length = len(train_labels)\n",
    "mid = int(0.8 * train_length)\n",
    "train_set = torch.utils.data.TensorDataset(train_index[0:mid], torch.tensor(train_labels[0:mid]).long())\n",
    "test_set = torch.utils.data.TensorDataset(test_index, torch.tensor(test_labels).long())\n",
    "dev_set = torch.utils.data.TensorDataset(train_index[mid:train_length], torch.tensor(train_labels[mid:train_length]).long())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29986,
     "status": "ok",
     "timestamp": 1590419472782,
     "user": {
      "displayName": "任一",
      "photoUrl": "",
      "userId": "04098430598358376154"
     },
     "user_tz": -480
    },
    "id": "bF0BzlJS9MCy",
    "outputId": "8c9ea10d-d594-4a71-b897-679b50a37f2a"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ku6Lp9Sr9MC2"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Net(nn.Module):\n",
    "    sentence_length = -1\n",
    "    conv_size = 64\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1 = nn.Conv2d(1, 1, (self.conv_size, embedding_dim))\n",
    "        self.fc = nn.Linear(1000 - self.conv_size + 1, 8)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "    def reset_sentence_length(self, length):\n",
    "      self.sentence_length = length\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x.unsqueeze(dim = 1)\n",
    "        x1 = nn.functional.relu(self.dropout(self.conv1(x)))\n",
    "        x = self.fc(x1.view(-1, 1000-self.conv_size+1))\n",
    "        x = nn.functional.softmax(x, dim = 1)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1pyLUQC9MC4"
   },
   "outputs": [],
   "source": [
    "def test_accuracy(loader, net, length):\n",
    "    '''\n",
    "    used in dev set accuracy calculating\n",
    "    '''\n",
    "    net.reset_sentence_length(length)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.train(False)\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(loader)):\n",
    "            # inputs, label = data\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0) # batch size\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / (0.0 + total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqCAnccuawcP"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "lr = 0.05\n",
    "batch_size = 16\n",
    "dev_patience = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3386,
     "status": "ok",
     "timestamp": 1590419610997,
     "user": {
      "displayName": "任一",
      "photoUrl": "",
      "userId": "04098430598358376154"
     },
     "user_tz": -480
    },
    "id": "kOBTfQI49MC6",
    "outputId": "dc50ce41-1f7b-40e8-e73a-e27db4569ead"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_set, batch_size=batch_size)\n",
    "print(vocab_size, embedding_dim)\n",
    "net = Net(vocab_size, embedding_dim)\n",
    "net.embeddings = nn.Embedding.from_pretrained(torch.load('./data/word_vec.pt'))\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=1e-3)\n",
    "model_path = './model/SimpleCNN_EMBED_cuda.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3376,
     "status": "ok",
     "timestamp": 1590419610998,
     "user": {
      "displayName": "任一",
      "photoUrl": "",
      "userId": "04098430598358376154"
     },
     "user_tz": -480
    },
    "id": "E1TpeWzVw9GZ",
    "outputId": "3c3fa9d2-df44-4370-94cc-d00e83fdc417"
   },
   "outputs": [],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_DcIqxu09MC8"
   },
   "outputs": [],
   "source": [
    "def check_dev_patience(acc_list, patience = 5):\n",
    "    if len(acc_list) <= patience:\n",
    "        return True\n",
    "    check_list = acc_list[len(acc_list)-patience:len(acc_list)]\n",
    "    for i in range(patience - 2):\n",
    "        if check_list[i] - check_list[i + 1] >= 0.01 and check_list[i + 1] - check_list[i + 2] >= 0.01:\n",
    "            return False\n",
    "    if np.std(check_list) < 1e-4:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4511,
     "status": "ok",
     "timestamp": 1590419612152,
     "user": {
      "displayName": "任一",
      "photoUrl": "",
      "userId": "04098430598358376154"
     },
     "user_tz": -480
    },
    "id": "F2RibVn89MC-",
    "outputId": "96b3a673-e623-4675-e007-5e4f4c0f50b9"
   },
   "outputs": [],
   "source": [
    "net.reset_sentence_length(test_sentence_length)\n",
    "print('before training test acc:', test_accuracy(test_loader, net, test_sentence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 231123,
     "status": "ok",
     "timestamp": 1590314053064,
     "user": {
      "displayName": "任一",
      "photoUrl": "",
      "userId": "04098430598358376154"
     },
     "user_tz": -480
    },
    "id": "LJrXPlNc9MDC",
    "outputId": "70bea5f4-dee1-4c19-b960-a6eb0f807f7a",
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "net.reset_sentence_length(train_sentence_length)\n",
    "epochs = 500\n",
    "loss_batch = train_length / batch_size / 3\n",
    "dev_acc_list = [0] # initialize with a zero, easy to compare before first dev acc comes in\n",
    "loss_list = []\n",
    "epoch_list = []\n",
    "trained_epoch_num = 0\n",
    "start_time = time()\n",
    "print('Start Training, lr=%f, bs=%d, embedding dim=%d'%(lr, batch_size, embedding_dim))\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    try:\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        cnt = 0\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # inputs, labels = data\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            net.train(True)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            epoch_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            cnt += 1\n",
    "            if i % loss_batch == loss_batch - 1:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                    (epoch + 1, i + 1, running_loss / loss_batch))\n",
    "                running_loss = 0.0\n",
    "\n",
    "        dev_accuracy = test_accuracy(dev_loader, net, train_sentence_length)\n",
    "        print('epoch %d loss: %.4f, dev acc = %.2f%%' % (epoch + 1, epoch_loss / cnt, dev_accuracy * 100))\n",
    "        if dev_accuracy > max(dev_acc_list):\n",
    "            print('New Model Saved!')\n",
    "            torch.save(net.state_dict(), model_path)\n",
    "        dev_acc_list.append(dev_accuracy)\n",
    "        loss_list.append(epoch_loss / cnt)\n",
    "        epoch_list.append(epoch + 1)\n",
    "        no_big_improve_on_dev = check_dev_patience(dev_acc_list, dev_patience)\n",
    "        if not no_big_improve_on_dev:\n",
    "            print('No significant improve on dev set, early stopped automatically!')\n",
    "            break\n",
    "        trained_epoch_num += 1\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "      break\n",
    "print('Finished Training, trained for %d epochs, total time = %.2fs, avg time per epoch = %.2fs' % (trained_epoch_num, time() - start_time, (time() - start_time) / trained_epoch_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ltAVKZ_V9MDF"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "net.load_state_dict(torch.load(model_path))\n",
    "net.reset_sentence_length(test_sentence_length)\n",
    "net.to(device)\n",
    "predicted_label = list()\n",
    "groundTruth_label = list()\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle = False)\n",
    "corr = 0.0\n",
    "\n",
    "net.train(False)\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(tqdm(test_loader)):\n",
    "        # inputs, labels = data\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = net(inputs)\n",
    "        groundTruth_label.extend(labels.tolist())\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_label.extend(predicted.tolist())\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        corr += pearsonr(outputs[0].cpu(), test_labels_distribution[i])[0]\n",
    "    corr /= test_sentence_length\n",
    "\n",
    "print('\\nAccuracy of the network: %.4f %%' % (100 * correct / total))\n",
    "print('Macro F1 score: %.2f'%f1_score(groundTruth_label, predicted_label, average='macro'))\n",
    "print('Micro F1 score: %.2f'%f1_score(groundTruth_label, predicted_label, average='micro'))\n",
    "print('Corr:', corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bihzL_SN9MDH"
   },
   "outputs": [],
   "source": [
    "acc_list = [100 * x for x in dev_acc_list[1:]]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "lns1 = ax.plot(epoch_list, acc_list, '-r', label='accuracy')\n",
    "ax2 = ax.twinx()\n",
    "lns2 = ax2.plot(epoch_list, loss_list, label = 'loss')\n",
    "lns = lns1 + lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax.legend(lns, labs, loc=0)\n",
    "ax.grid()\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('dev accuracy / %')\n",
    "ax2.set_ylabel('loss')\n",
    "ax.set_ylim(max(min(acc_list) - 10, 0), 100)\n",
    "ax2.set_ylim(np.mean(loss_list) - 2 * np.std(loss_list), np.mean(loss_list) + 2 * np.std(loss_list))\n",
    "plt.title('dev accuracy&loss epoch=%d lr=%f bs=%d maxacc=%.2f%%'%(max(epoch_list),lr,batch_size,max(acc_list)))\n",
    "# plt.savefig('./data/TCNNEpoch%dlr%fbatchsize%dmaxacc%.2f.jpg' % (max(epoch_list), lr, batch_size, max(acc_list)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dpP53ueF9MDJ"
   },
   "outputs": [],
   "source": [
    "# print(predicted_label)\n",
    "# print(groundTruth_label)\n",
    "# test_accuracy(train_loader, net, train_sentence_length)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TEXTCNN_pretrain_cuda.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}